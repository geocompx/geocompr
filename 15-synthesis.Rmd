# Conclusion {#conclusion}

## Prerequisites {-}

Like the introduction, this concluding chapter contains few code chunks.
But its prerequisites are demanding.
It assumes that you have:

- Read-through and attempted the exercises in all the chapters of Part 1 (Foundations).
- Grasped the diversity of methods that build on these foundations, by following the code and prose in Part 2 (Extensions).
- Considered how you can use geocomputation to solve real-world problems, at work and beyond, after engaging with Part 3 (Applications).

The aim is to consolidate knowledge and skills for geocomputation and inspire future directions of application and development.
<!-- Section \@ref(concepts) reviews the content covered in the previous chapters but at a high level. -->
<!-- Previous chapters focus on the details of packages, functions, and arguments needed for geocomputation with R. -->
<!-- This chapter focuses on concepts that recur throughout the book and how they may be useful. -->
Section \@ref(gaps) discusses gaps in the book's contents and explains why some areas of research were deliberately omitted while others were emphasized.
This discussion leads to the question (which is answered in section \@ref(next)): having read this book, where next?
The final section (\@ref(benefit)) returns to the wider issues raised in Chapter \@ref(intro) and considers how geocomputation can be used for social benefit.

<!-- Section \@ref(next) -->

<!-- ## Concepts for geocomputation {#concepts} -->

## Gaps and overlaps {#gaps}

A characteristic of R is that there are often multiple ways to achieve the same result.
Geocomputation with R is no exception.
The code chunk below illustrates this by using three functions covered in Chapters \@ref(attr) and \@ref(geometric-operations)
<!-- from 3 packages (**sf**, base R's **stats** and tidyverse's **dplyr**) -->
based on the **sf** package to combine the 16 regions of New Zealand into a single geometry:

```{r}
library(spData)
nz_u1 = sf::st_union(nz)
nz_u2 = aggregate(nz["Population"], list(rep(1, nrow(nz))), sum)
nz_u3 = dplyr::summarise(nz, t = sum(Population))
identical(nz_u1, nz_u2$geometry)
identical(nz_u1, nz_u3$geom)
```

Although the classes, attributes and column names of the resulting objects `nz_u1` to `nz_u3` differ, their geometries are identical.
This is verified using the base R function `identical()`.^[
The first operation, undertaken by the function `st_union()`, creates an object of class `sfc` (a simple feature column).
The latter two operations create `sf` objects, each of which *contains* a simple feature column.
Therefore it is the geometries contained in simple feature columns, not the objects themselves, that are identical.
]
Which to use?
It depends.
The former only processes the geometry data contained in `nz` so is faster.
Latter two options performed attribute operations, which may be useful for subsequent steps.
Choosing between base R and the tidyverse is largely a matter of preference (see Chapter \@ref(attr)), although there are a number of pitfalls to avoid when using **tidyverse** functions to handle spatial data (see the supplementary article `spatial-tidyverse` at [geocompr.github.io](https://geocompr.github.io/)).

Another area of overlap is *between* packages.
Chapter \@ref(intro) mentions 20+ influential spatial packages that have been developed over the years.
Although each package covered in this book has a different emphasis, there are overlaps between them.
And there are dozens of packages for geographic data *not* covered in this book.
There are 176 packages in the Spatial [Task View](https://cran.r-project.org/web/views/) alone (as of summer 2018); more packages and countless functions for geographic data are developed each year.

```{r, eval=FALSE, echo=FALSE}
# aim: find number of packages in the spatial task view
# how? see:
# vignette("selectorgadget")
stv_pkgs = xml2::read_html("https://cran.r-project.org/web/views/Spatial.html")
pkgs = rvest::html_nodes(stv_pkgs, "ul:nth-child(5) a")
pkgs_char = rvest::html_text(pkgs)
length(pkgs_char)
```

The volume and constant evolution of packages and functions for geographic data makes it practically impossible to keep up with all developments in the broadly defined 'R-spatial' community (section \@ref(next) covers developments in other languages).
There is much overlap and some packages perform much better than others, making package selection an important decision.
From this diversity, we have chosen packages that we believe are worth learning because they are future-proof (i.e. they will be maintained into the future), high performance (relative to other R packages) and complimentary.
But there is still much overlap between them, as illustrated by the diversity of packages in the field of geographic data visualization alone (see Chapter \@ref(adv-map)).
Overlaps between packages are not a bad thing: they ensure the resilience of the wider R ecosystem and provide choice.
Being able to choose between multiple options is a key feature of open source software.
When using a particular 'stack', such as the **sf**/**tidyverse**/**raster** ecosystem advocated in this book, it is worth being aware of alternatives already developed (such as **sp**/**rgdal**/**rgeos**) and, where possible, promissing alternatives that are under development (such as **stars**).

Likewise, there are gaps and overlaps in the contents of this book, which are worth considering before we consider next steps in section \@ref(next).

<!-- More than 15 years ago, before most of the packages used in this book had been developed,  -->

## Where next? {#next}

Learning geocomputation with R is challenging and there is much more to discover.
We have progressed quickly.
The jump from chapters \@ref(spatial-class) to \@ref(eco) is large:
the creation and manipulation of simple spatial objects in Part I may seem a world away from the analysis of large datasets covered in Part III.
It is impossible to become an expert in any area by reading a single book, and skills must be practiced.
This section points toward sensible next steps on your geocomputational journey, **highlighted in bold below**.
<!-- and ordered by difficulty, beginning with continue to improve your knowledge of R. -->

The language of R is a shared thread connecting all the chapters.
All the analyses are based on R classes, primarily `sf` and `raster`, which in turn build on the base R classes of `data.frame` and `matrix`.
The wider point is the importance of *depth of understanding*.
This observation suggests a direction of travel: **improving your understanding of the R language**.
A next step in this direction is to deepen your knowledge of base R, for example by: studying R's key documents (which can be found by entering `help.start()` in the console), reading and playing with the source code of useful functions, or reading comprehensive resources on the subject such those by @wickham_advanced_2014 and @chambers_extending_2016.
<!-- creating and querying simple spatial in Chapter \ref(spatial-class) -->
<!-- maybe we should add info about places to learn more r-spatial stuff (aka github, twitter, ...?)? -->

<!-- Many directions of travel could be taken after taking the geocomputational steps -->
Perhaps the most obvious direction for future learning is **discovering geocomputation with other languages**.
There are good reasons for learning R as a language for geocomputation, as described in Chapter \@ref(intro), but it is not the only option.^[
R's strengths relevant to our definition geocomputation include its emphasis on scientific reproducibility, widespread use in academic research and unparalleled support for statistical modeling of geographic data.
Furthermore, we advocate learning one language (R) for geocomputation in depth before delving into other languages/frameworks because of the costs associated with context switching.
It is preferable to have expertise in one language than basic knowledge of many.
]
It is possible to study *Geocomputation with Python*, *C++*, *JavaScript*, *Scala*, *Julia* or *Rust* in equal depth.
Each of these is a promising language for geocomputation.
[**Turf.js**](https://github.com/Turfjs/turf), for example, provides many functions for geospatial analysis with implementations in JavaScript, Julia, and even Swift.
[**rasterio**](https://github.com/mapbox/rasterio) is a Python package for raster offering a high-performance interface to GDAL for handling raster data.
These and other tantalizing geospatial software projects can be found on the GitHub repo [Awesome-Geospatial](https://github.com/sacridini/Awesome-Geospatial).

<!-- misc ideas: -->
<!-- - learning the geocomputation history (e.g. great papers by S. Openshaw) -->
<!-- - learning about new geocomputation methods (not implemented) -->
<!-- - reading about new non-spatial methods and be inspired (e.g. from fields of image analysis or geometry) -->
<!-- - combining methods from outside R with R -->
<!-- - creating new methods (reference to ch 10) -->

## Geo* for social benefit {#benefit}

This is a technical book so it makes sense for the next steps to also be technical.
But there are many non-technical issues to consider, now you understand what is possible with geographic data in R.
This section returns to the defintion of geocomputation and wider issues covered in Chapter \@ref(intro).
It argues for the methods to be used to tackle some of the planet's most pressing problems.
The use of geo* rather than geocomputation is deliberate.
Many terms, including geographic data science, geographic information systems and geoinformatics, capture the range of possibilities opened-up by geospatial software and knowledge of data.
But geocomputation has advantages: a concise term that defines a field with with three main ingredients:

- The *creative* use of geographic data.
- Application to *real-world problems* for social benefit.
- Building tools in the context of a 'scientific' approach @openshaw_geocomputation_2000.

Only the final ingredient is technical.
We believe the broader non-technical aims are what make geospatial work so rewarding.
what is the point of building a new geographic method (tool) if its only purpose is to increase sales of perfume?
<!-- BOOM! None. -->

<!-- A bit of a rapid jump to reproducibility, I suggest another paragraph goes before this one (RL) -->
Reproducibility is an additional ingredient that can ensure geo* work is socially beneficial, or at least benign.
It supports *creativity*, encouraging the focus of methods to shift away from the basics (which are readily available through shared code, avoiding many people 'reinventing the wheel') and toward applications.
<!-- that nobody has yet thought of. -->
Reproducibility encourages geocomputation for social benefit because it makes geographic data analysis publicly accessible and transparent.

The benefits of reproducibility can be illustrated with the example of using geocomputation to increase sales of perfume.
If the methods are hidden and cannot reproduced, few people can benefit (except perhaps perfume companies!).
If the underlying code is made open and reproducible, by contrast, the methods can be re-purposed.
Reproducibility encourages socially beneficial collaboration.^[
One accessible way to contribute upstream is creating a reprex (reproducible example) to highlight a bug in the package's issue tracker, as outlined in section \@ref(scripts).
]
The importance of reproducibility, and other non-technical ingredients of geocompuations, are further discussed in an open access artical celebrating '21+ years of geocomputation' [@harris_more_2017].

<!-- Like any worthwhile intellectual endeavor or nascent academic field, geocomputation is diverse and contested. -->
