<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 12 Statistical learning | Geocomputation with R</title>
<meta name="author" content="Robin Lovelace, Jakub Nowosad, Jannes Muenchow">
<meta name="description" content="Prerequisites This chapter assumes proficiency with geographic data analysis, for example gained by studying the contents and working-through the exercises in Chapters 2 to 7. A familiarity with...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 12 Statistical learning | Geocomputation with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://geocompr.robinlovelace.net/spatial-cv.html">
<meta property="og:image" content="https://geocompr.robinlovelace.net/images/cover.png">
<meta property="og:description" content="Prerequisites This chapter assumes proficiency with geographic data analysis, for example gained by studying the contents and working-through the exercises in Chapters 2 to 7. A familiarity with...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 12 Statistical learning | Geocomputation with R">
<meta name="twitter:description" content="Prerequisites This chapter assumes proficiency with geographic data analysis, for example gained by studying the contents and working-through the exercises in Chapters 2 to 7. A familiarity with...">
<meta name="twitter:image" content="https://geocompr.robinlovelace.net/images/cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><link href="libs/Lato-0.4.2/font.css" rel="stylesheet">
<link href="libs/Roboto_Mono-0.4.2/font.css" rel="stylesheet">
<link href="libs/Montserrat-0.4.2/font.css" rel="stylesheet">
<script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><meta name="citation_title" content="Chapter 12 Statistical learning | Geocomputation with R">
<meta name="citation_author" content="Robin Lovelace">
<meta name="citation_author" content="Jakub Nowosad">
<meta name="citation_author" content="Jannes Muenchow">
<meta name="citation_publication_date" content="2019">
<meta name="citation_isbn" content="9780203730058">
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet">
<script src="libs/leaflet-1.3.1/leaflet.js"></script><link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet">
<script src="libs/proj4-2.6.2/proj4.min.js"></script><script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script><link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet">
<script src="libs/leaflet-binding-2.1.1/leaflet.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-99618359-1', 'auto');
      ga('send', 'pageview');

    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h2>
        <a href="index.html" title="">Geocomputation with R</a>
      </h2>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="foreword-1st-edition.html">Foreword (1st Edition)</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Foundations</li>
<li><a class="" href="spatial-class.html"><span class="header-section-number">2</span> Geographic data in R</a></li>
<li><a class="" href="attr.html"><span class="header-section-number">3</span> Attribute data operations</a></li>
<li><a class="" href="spatial-operations.html"><span class="header-section-number">4</span> Spatial data operations</a></li>
<li><a class="" href="geometric-operations.html"><span class="header-section-number">5</span> Geometry operations</a></li>
<li><a class="" href="raster-vector.html"><span class="header-section-number">6</span> Raster-vector interactions</a></li>
<li><a class="" href="reproj-geo-data.html"><span class="header-section-number">7</span> Reprojecting geographic data</a></li>
<li><a class="" href="read-write.html"><span class="header-section-number">8</span> Geographic data I/O</a></li>
<li class="book-part">Extensions</li>
<li><a class="" href="adv-map.html"><span class="header-section-number">9</span> Making maps with R</a></li>
<li><a class="" href="gis.html"><span class="header-section-number">10</span> Bridges to GIS software</a></li>
<li><a class="" href="algorithms.html"><span class="header-section-number">11</span> Scripts, algorithms and functions</a></li>
<li><a class="active" href="spatial-cv.html"><span class="header-section-number">12</span> Statistical learning</a></li>
<li class="book-part">Applications</li>
<li><a class="" href="transport.html"><span class="header-section-number">13</span> Transportation</a></li>
<li><a class="" href="location.html"><span class="header-section-number">14</span> Geomarketing</a></li>
<li><a class="" href="eco.html"><span class="header-section-number">15</span> Ecology</a></li>
<li><a class="" href="conclusion.html"><span class="header-section-number">16</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/Robinlovelace/geocompr">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="spatial-cv" class="section level1" number="12">
<h1>
<span class="header-section-number">12</span> Statistical learning<a class="anchor" aria-label="anchor" href="#spatial-cv"><i class="fas fa-link"></i></a>
</h1>
<div id="prerequisites-9" class="section level2 unnumbered">
<h2>Prerequisites<a class="anchor" aria-label="anchor" href="#prerequisites-9"><i class="fas fa-link"></i></a>
</h2>
<p>This chapter assumes proficiency with geographic data analysis, for example gained by studying the contents and working-through the exercises in Chapters <a href="spatial-class.html#spatial-class">2</a> to <a href="reproj-geo-data.html#reproj-geo-data">7</a>.
A familiarity with generalized linear models (GLM) and machine learning is highly recommended <span class="citation">(for example from <a href="references.html#ref-zuur_mixed_2009" role="doc-biblioref">A. Zuur et al. 2009</a>; <a href="references.html#ref-james_introduction_2013" role="doc-biblioref">James et al. 2013</a>)</span>.</p>
<p>The chapter uses the following packages:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;
Packages &lt;strong&gt;GGally&lt;/strong&gt;, &lt;strong&gt;lgr&lt;/strong&gt;, &lt;strong&gt;kernlab&lt;/strong&gt;, &lt;strong&gt;ml3measures&lt;/strong&gt;, &lt;strong&gt;paradox&lt;/strong&gt;, &lt;strong&gt;pROC&lt;/strong&gt;, &lt;strong&gt;progressr&lt;/strong&gt; and &lt;strong&gt;spDataLarge&lt;/strong&gt; must also be installed although these do not need to be attached.&lt;/p&gt;"><sup>74</sup></a></p>
<div class="sourceCode" id="cb398"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://future.futureverse.org">future</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://s-fleck.github.io/lgr/">lgr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3.mlr-org.com">mlr3</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3learners.mlr-org.com">mlr3learners</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">mlr3extralearners</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3spatiotempcv.mlr-org.com/">mlr3spatiotempcv</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3tuning.mlr-org.com">mlr3tuning</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3viz.mlr-org.com">mlr3viz</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://progressr.futureverse.org">progressr</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r-spatial.github.io/sf/">sf</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://rspatial.org/terra/">terra</a></span><span class="op">)</span></span></code></pre></div>
<p>Required data will be attached in due course.</p>
</div>
<div id="intro-cv1" class="section level2" number="12.1">
<h2>
<span class="header-section-number">12.1</span> Introduction<a class="anchor" aria-label="anchor" href="#intro-cv1"><i class="fas fa-link"></i></a>
</h2>
<p>Statistical learning is concerned with the use of statistical and computational models for identifying patterns in data and predicting from these patterns.
Due to its origins, statistical learning is one of R’s great strengths (see Section <a href="intro.html#software-for-geocomputation">1.3</a>).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;
Applying statistical techniques to geographic data has been an active topic of research for many decades in the fields of geostatistics, spatial statistics and point pattern analysis &lt;span class="citation"&gt;(&lt;a href="references.html#ref-diggle_modelbased_2007" role="doc-biblioref"&gt;Diggle and Ribeiro 2007&lt;/a&gt;; &lt;a href="references.html#ref-gelfand_handbook_2010" role="doc-biblioref"&gt;Gelfand et al. 2010&lt;/a&gt;; &lt;a href="references.html#ref-baddeley_spatial_2015" role="doc-biblioref"&gt;Baddeley, Rubak, and Turner 2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>75</sup></a>
Statistical learning combines methods from statistics and machine learning and can be categorized into supervised and unsupervised techniques.
Both are increasingly used in disciplines ranging from physics, biology and ecology to geography and economics <span class="citation">(<a href="references.html#ref-james_introduction_2013" role="doc-biblioref">James et al. 2013</a>)</span>.</p>
<p>This chapter focuses on supervised techniques in which there is a training dataset, as opposed to unsupervised techniques such as clustering.
Response variables can be binary (such as landslide occurrence), categorical (land use), integer (species richness count) or numeric (soil acidity measured in pH).
Supervised techniques model the relationship between such responses — which are known for a sample of observations — and one or more predictors.</p>
<!-- For this we can use techniques from the field of statistics or from the field of machine learning.
Which to use depends on the primary aim: statistical inference or prediction.
Statistical regression techniques are especially useful if the aim is statistical inference.
These techniques also allow predictions of unseen data points but this is usually only of secondary interest to statisticians.
Statistical inference, on the other hand, refers among others to a predictor's significance, its importance for a specific model, its relationship with the response and the uncertainties associated with the estimated coefficients.
To trust the p-values and standard errors of such models we need to perform a thorough model validation testing if one or several of the underlying model assumptions (heterogeneity, independence, etc.) have been violated [@zuur_mixed_2009].
By contrast, statistical inference is impossible with machine learning [@james_introduction_2013].
-->
<!-- The primary aim of machine learning is to make good predictions, whereas the field of statistics is more focussed on the underlying theory [e.g. @zuur_mixed_2009] -->
<p>The primary aim of much machine learning research is to make good predictions, as opposed to statistical/Bayesian inference, which is good at helping to understand underlying mechanisms and uncertainties in the data <span class="citation">(see <a href="references.html#ref-krainski_advanced_2018" role="doc-biblioref">Krainski et al. 2018</a>)</span>.
Machine learning thrives in the age of ‘big data’ because its methods make few assumptions about input variables and can handle huge datasets.
Machine learning is conducive to tasks such as the prediction of future customer behavior, recommendation services (music, movies, what to buy next), face recognition, autonomous driving, text classification and predictive maintenance (infrastructure, industry).</p>
<!-- ^[In this case we do not have too worry too much about possible model misspecifications since we explicitly do not want to do statistical inference.] -->
<p>This chapter is based on a case study: the (spatial) prediction of landslides.
This application links to the applied nature of geocomputation, defined in Chapter <a href="intro.html#intro">1</a>, and illustrates how machine learning borrows from the field of statistics when the sole aim is prediction.
Therefore, this chapter first introduces modeling and cross-validation concepts with the help of a Generalized Linear Model <span class="citation">(<a href="references.html#ref-zuur_mixed_2009" role="doc-biblioref">A. Zuur et al. 2009</a>)</span>.
Building on this, the chapter implements a more typical machine learning algorithm, namely a Support Vector Machine (SVM).
The models’ <strong>predictive performance</strong> will be assessed using spatial cross-validation (CV), which accounts for the fact that geographic data is special.</p>
<p>CV determines a model’s ability to generalize to new data, by splitting a dataset (repeatedly) into training and test sets.
It uses the training data to fit the model, and checks its performance when predicting against the test data.
CV helps to detect overfitting since models that predict the training data too closely (noise) will tend to perform poorly on the test data.</p>
<p>Randomly splitting spatial data can lead to training points that are neighbors in space with test points.
Due to spatial autocorrelation, test and training datasets would not be independent in this scenario, with the consequence that CV fails to detect a possible overfitting.
Spatial CV alleviates this problem and is the <strong>central</strong> theme in this chapter.</p>
</div>
<div id="case-landslide" class="section level2" number="12.2">
<h2>
<span class="header-section-number">12.2</span> Case study: Landslide susceptibility<a class="anchor" aria-label="anchor" href="#case-landslide"><i class="fas fa-link"></i></a>
</h2>
<p>This case study is based on a dataset of landslide locations in Southern Ecuador, illustrated in Figure <a href="spatial-cv.html#fig:lsl-map">12.1</a> and described in detail in <span class="citation">Muenchow, Brenning, and Richter (<a href="references.html#ref-muenchow_geomorphic_2012" role="doc-biblioref">2012</a>)</span>.
A subset of the dataset used in that paper is provided in the <strong>spDataLarge</strong> package, which can be loaded as follows:</p>
<div class="sourceCode" id="cb399"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"lsl"</span>, <span class="st">"study_mask"</span>, package <span class="op">=</span> <span class="st">"spDataLarge"</span><span class="op">)</span></span>
<span><span class="va">ta</span> <span class="op">=</span> <span class="fu">terra</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/terra/man/rast.html">rast</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/system.file.html">system.file</a></span><span class="op">(</span><span class="st">"raster/ta.tif"</span>, package <span class="op">=</span> <span class="st">"spDataLarge"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The above code loads three objects: a <code>data.frame</code> named <code>lsl</code>, an <code>sf</code> object named <code>study_mask</code> and a <code>SpatRaster</code> (see Section <a href="spatial-class.html#raster-classes">2.3.4</a>) named <code>ta</code> containing terrain attribute rasters.
<code>lsl</code> contains a factor column <code>lslpts</code> where <code>TRUE</code> corresponds to an observed landslide ‘initiation point’, with the coordinates stored in columns <code>x</code> and <code>y</code>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;
The landslide initiation point is located in the scarp of a landslide polygon. See &lt;span class="citation"&gt;Muenchow, Brenning, and Richter (&lt;a href="references.html#ref-muenchow_geomorphic_2012" role="doc-biblioref"&gt;2012&lt;/a&gt;)&lt;/span&gt; for further details.&lt;/p&gt;'><sup>76</sup></a>
There are 175 landslide and 175 non-landslide points, as shown by <code>summary(lsl$lslpts)</code>.
The 175 non-landslide points were sampled randomly from the study area, with the restriction that they must fall outside a small buffer around the landslide polygons.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:lsl-map"></span>
<img src="figures/lsl-map-1.png" alt="Landslide initiation points (red) and points unaffected by landsliding (blue) in Southern Ecuador." width="70%"><p class="caption">
FIGURE 12.1: Landslide initiation points (red) and points unaffected by landsliding (blue) in Southern Ecuador.
</p>
</div>
<p></p>
<p>The first three rows of <code>lsl</code>, rounded to two significant digits, can be found in Table <a href="spatial-cv.html#tab:lslsummary">12.1</a>.</p>
<div class="inline-table"><table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:lslsummary">TABLE 12.1: </span>Structure of the lsl dataset.
</caption>
<thead><tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
<th style="text-align:left;">
lslpts
</th>
<th style="text-align:right;">
slope
</th>
<th style="text-align:right;">
cplan
</th>
<th style="text-align:right;">
cprof
</th>
<th style="text-align:right;">
elev
</th>
<th style="text-align:right;">
log10_carea
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:right;">
713888
</td>
<td style="text-align:right;">
9558537
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
0.023
</td>
<td style="text-align:right;">
0.003
</td>
<td style="text-align:right;">
2400
</td>
<td style="text-align:right;">
2.8
</td>
</tr>
<tr>
<td style="text-align:left;">
2
</td>
<td style="text-align:right;">
712788
</td>
<td style="text-align:right;">
9558917
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:right;">
39
</td>
<td style="text-align:right;">
-0.039
</td>
<td style="text-align:right;">
-0.017
</td>
<td style="text-align:right;">
2100
</td>
<td style="text-align:right;">
4.1
</td>
</tr>
<tr>
<td style="text-align:left;">
350
</td>
<td style="text-align:right;">
713826
</td>
<td style="text-align:right;">
9559078
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
0.020
</td>
<td style="text-align:right;">
-0.003
</td>
<td style="text-align:right;">
2400
</td>
<td style="text-align:right;">
3.2
</td>
</tr>
</tbody>
</table></div>
<p>To model landslide susceptibility, we need some predictors.
Since terrain attributes are frequently associated with landsliding <span class="citation">(<a href="references.html#ref-muenchow_geomorphic_2012" role="doc-biblioref">Muenchow, Brenning, and Richter 2012</a>)</span>, we have already extracted following terrain attributes from <code>ta</code> to <code>lsl</code>:</p>
<ul>
<li>
<code>slope</code>: slope angle (°)</li>
<li>
<code>cplan</code>: plan curvature (rad m<sup>−1</sup>) expressing the convergence or divergence of a slope and thus water flow</li>
<li>
<code>cprof</code>: profile curvature (rad m<sup>-1</sup>) as a measure of flow acceleration, also known as downslope change in slope angle</li>
<li>
<code>elev</code>: elevation (m a.s.l.) as the representation of different altitudinal zones of vegetation and precipitation in the study area</li>
<li>
<code>log10_carea</code>: the decadic logarithm of the catchment area (log10 m<sup>2</sup>) representing the amount of water flowing towards a location</li>
</ul>
<p>It might be a worthwhile exercise to compute the terrain attributes with the help of R-GIS bridges (see Chapter <a href="gis.html#gis">10</a>) and extract them to the landslide points (see Exercise section at the end of this Chapter).</p>
</div>
<div id="conventional-model" class="section level2" number="12.3">
<h2>
<span class="header-section-number">12.3</span> Conventional modeling approach in R<a class="anchor" aria-label="anchor" href="#conventional-model"><i class="fas fa-link"></i></a>
</h2>
<p>Before introducing the <strong>mlr3</strong> package, an umbrella-package providing a unified interface to dozens of learning algorithms (Section <a href="spatial-cv.html#spatial-cv-with-mlr3">12.5</a>), it is worth taking a look at the conventional modeling interface in R.
This introduction to supervised statistical learning provides the basis for doing spatial CV, and contributes to a better grasp on the <strong>mlr3</strong> approach presented subsequently.</p>
<p>Supervised learning involves predicting a response variable as a function of predictors (Section <a href="spatial-cv.html#intro-cv">12.4</a>).
In R, modeling functions are usually specified using formulas (see <code><a href="https://rdrr.io/r/stats/formula.html">?formula</a></code> and the detailed <a href="https://www.datacamp.com/community/tutorials/r-formula-tutorial">Formulas in R Tutorial</a> for details of R formulas).
The following command specifies and runs a generalized linear model:</p>
<div class="sourceCode" id="cb400"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">lslpts</span> <span class="op">~</span> <span class="va">slope</span> <span class="op">+</span> <span class="va">cplan</span> <span class="op">+</span> <span class="va">cprof</span> <span class="op">+</span> <span class="va">elev</span> <span class="op">+</span> <span class="va">log10_carea</span>,</span>
<span>          family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>          data <span class="op">=</span> <span class="va">lsl</span><span class="op">)</span></span></code></pre></div>
<p>It is worth understanding each of the three input arguments:</p>
<ul>
<li>A formula, which specifies landslide occurrence (<code>lslpts</code>) as a function of the predictors</li>
<li>A family, which specifies the type of model, in this case <code>binomial</code> because the response is binary (see <code><a href="https://rdrr.io/r/stats/family.html">?family</a></code>)</li>
<li>The data frame which contains the response and the predictors (as columns)</li>
</ul>
<p>The results of this model can be printed as follows (<code>summary(fit)</code> provides a more detailed account of the results):</p>
<div class="sourceCode" id="cb401"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "glm" "lm"</span></span>
<span><span class="va">fit</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:  glm(formula = lslpts ~ slope + cplan + cprof + elev + log10_carea, </span></span>
<span><span class="co">#&gt;     family = binomial(), data = lsl)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt; (Intercept)        slope        cplan        cprof         elev  log10_carea  </span></span>
<span><span class="co">#&gt;    2.51e+00     7.90e-02    -2.89e+01    -1.76e+01     1.79e-04    -2.27e+00  </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Degrees of Freedom: 349 Total (i.e. Null);  344 Residual</span></span>
<span><span class="co">#&gt; Null Deviance:       485 </span></span>
<span><span class="co">#&gt; Residual Deviance: 373   AIC: 385</span></span></code></pre></div>
<p>The model object <code>fit</code>, of class <code>glm</code>, contains the coefficients defining the fitted relationship between response and predictors.
It can also be used for prediction.
This is done with the generic <code><a href="https://rdrr.io/pkg/terra/man/predict.html">predict()</a></code> method, which in this case calls the function <code><a href="https://rdrr.io/r/stats/predict.glm.html">predict.glm()</a></code>.
Setting <code>type</code> to <code>response</code> returns the predicted probabilities (of landslide occurrence) for each observation in <code>lsl</code>, as illustrated below (see <code><a href="https://rdrr.io/r/stats/predict.glm.html">?predict.glm</a></code>):</p>
<div class="sourceCode" id="cb402"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">pred_glm</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/terra/man/predict.html">predict</a></span><span class="op">(</span>object <span class="op">=</span> <span class="va">fit</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/terra/man/headtail.html">head</a></span><span class="op">(</span><span class="va">pred_glm</span><span class="op">)</span></span>
<span><span class="co">#&gt;      1      2      3      4      5      6 </span></span>
<span><span class="co">#&gt; 0.1901 0.1172 0.0952 0.2503 0.3382 0.1575</span></span></code></pre></div>
<p>Spatial predictions can be made by applying the coefficients to the predictor rasters.
This can be done manually or with <code><a href="https://rdrr.io/pkg/terra/man/predict.html">terra::predict()</a></code>.
In addition to a model object (<code>fit</code>), this function also expects a <code>SpatRaster</code> with the predictors (raster layers) named as in the model’s input data frame (Figure <a href="spatial-cv.html#fig:lsl-susc">12.2</a>).</p>
<div class="sourceCode" id="cb403"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># making the prediction</span></span>
<span><span class="va">pred</span> <span class="op">=</span> <span class="fu">terra</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/terra/man/predict.html">predict</a></span><span class="op">(</span><span class="va">ta</span>, model <span class="op">=</span> <span class="va">fit</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:lsl-susc"></span>
<img src="figures/lsl-susc-1.png" alt="Spatial prediction of landslide susceptibility using a GLM." width="70%"><p class="caption">
FIGURE 12.2: Spatial prediction of landslide susceptibility using a GLM.
</p>
</div>
<p>Here, when making predictions we neglect spatial autocorrelation since we assume that on average the predictive accuracy remains the same with or without spatial autocorrelation structures.
However, it is possible to include spatial autocorrelation structures into models <span class="citation">(<a href="references.html#ref-zuur_mixed_2009" role="doc-biblioref">A. Zuur et al. 2009</a>; <a href="references.html#ref-blangiardo_spatial_2015" role="doc-biblioref">Blangiardo and Cameletti 2015</a>; <a href="references.html#ref-zuur_beginners_2017" role="doc-biblioref">A. F. Zuur et al. 2017</a>)</span> as well as into predictions <span class="citation">(kriging approaches, see, e.g., <a href="references.html#ref-goovaerts_geostatistics_1997" role="doc-biblioref">Goovaerts 1997</a>; <a href="references.html#ref-hengl_practical_2007" role="doc-biblioref">Tomislav Hengl 2007</a>; <a href="references.html#ref-bivand_applied_2013" role="doc-biblioref">Bivand, Pebesma, and Gómez-Rubio 2013</a>)</span>.
This is, however, beyond the scope of this book.
<!--
Nevertheless, we give the interested reader some pointers where to look it up:

1. The predictions of regression kriging combines the predictions of a regression with the kriging of the regression's residuals [@bivand_applied_2013]. 
2. One can also add a spatial correlation (dependency) structure to a generalized least squares model  [`nlme::gls()`; @zuur_mixed_2009; @zuur_beginners_2017].  
3. Finally, there are mixed-effect modeling approaches.
Basically, a random effect imposes a dependency structure on the response variable which in turn allows for observations of one class to be more similar to each other than to those of another class [@zuur_mixed_2009]. 
Classes can be, for example, bee hives, owl nests, vegetation transects or an altitudinal stratification.
This mixed modeling approach assumes normal and independent distributed random intercepts.^[Note that for spatial predictions one would usually use the population intercept.]
This can even be extended by using a random intercept that is normal and spatially dependent.
For this, however, you will have to resort most likely to Bayesian modeling approaches since frequentist software tools are rather limited in this respect especially for more complex models [@blangiardo_spatial_2015; @zuur_beginners_2017]. 
--></p>
<p>Spatial prediction maps are one very important outcome of a model.
Even more important is how good the underlying model is at making them since a prediction map is useless if the model’s predictive performance is bad.
The most popular measure to assess the predictive performance of a binomial model is the Area Under the Receiver Operator Characteristic Curve (AUROC).
This is a value between 0.5 and 1.0, with 0.5 indicating a model that is no better than random and 1.0 indicating perfect prediction of the two classes.
Thus, the higher the AUROC, the better the model’s predictive power.
The following code chunk computes the AUROC value of the model with <code>roc()</code>, which takes the response and the predicted values as inputs.
<code>auc()</code> returns the area under the curve.</p>
<div class="sourceCode" id="cb404"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">pROC</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/auc.html">auc</a></span><span class="op">(</span><span class="fu">pROC</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/pROC/man/roc.html">roc</a></span><span class="op">(</span><span class="va">lsl</span><span class="op">$</span><span class="va">lslpts</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Area under the curve: 0.8216</span></span></code></pre></div>
<p>An AUROC value of
<!--  -->
0.82 represents a good fit.
However, this is an overoptimistic estimation since we have computed it on the complete dataset.
To derive a biased-reduced assessment, we have to use cross-validation and in the case of spatial data should make use of spatial CV.</p>
</div>
<div id="intro-cv" class="section level2" number="12.4">
<h2>
<span class="header-section-number">12.4</span> Introduction to (spatial) cross-validation<a class="anchor" aria-label="anchor" href="#intro-cv"><i class="fas fa-link"></i></a>
</h2>
<p>Cross-validation belongs to the family of resampling methods <span class="citation">(<a href="references.html#ref-james_introduction_2013" role="doc-biblioref">James et al. 2013</a>)</span>.
The basic idea is to split (repeatedly) a dataset into training and test sets whereby the training data is used to fit a model which then is applied to the test set.
Comparing the predicted values with the known response values from the test set (using a performance measure such as the AUROC in the binomial case) gives a bias-reduced assessment of the model’s capability to generalize the learned relationship to independent data.
For example, a 100-repeated 5-fold cross-validation means to randomly split the data into five partitions (folds) with each fold being used once as a test set (see upper row of Figure <a href="spatial-cv.html#fig:partitioning">12.3</a>).
This guarantees that each observation is used once in one of the test sets, and requires the fitting of five models.
Subsequently, this procedure is repeated 100 times.
Of course, the data splitting will differ in each repetition.
<!--if the error is calc. on the fold-level. most often its calc. on the repetition level. maybe worth noting.
talk about this in person
-->
Overall, this sums up to 500 models, whereas the mean performance measure (AUROC) of all models is the model’s overall predictive power.</p>
<p>However, geographic data is special.
As we will see in Chapter <a href="transport.html#transport">13</a>, the ‘first law’ of geography states that points close to each other are, generally, more similar than points further away <span class="citation">(<a href="references.html#ref-miller_tobler_2004" role="doc-biblioref">Miller 2004</a>)</span>.
This means these points are not statistically independent because training and test points in conventional CV are often too close to each other (see first row of Figure <a href="spatial-cv.html#fig:partitioning">12.3</a>).
‘Training’ observations near the ‘test’ observations can provide a kind of ‘sneak preview’:
information that should be unavailable to the training dataset.
<!-- "folds" only for the repetition split, "partitions" or "subsets" for splitting within a fold
talk about this in person
-->
To alleviate this problem ‘spatial partitioning’ is used to split the observations into spatially disjointed subsets (using the observations’ coordinates in a <em>k</em>-means clustering; <span class="citation">Brenning (<a href="references.html#ref-brenning_spatial_2012" role="doc-biblioref">2012b</a>)</span>; second row of Figure <a href="spatial-cv.html#fig:partitioning">12.3</a>).
This partitioning strategy is the <strong>only</strong> difference between spatial and conventional CV.
As a result, spatial CV leads to a bias-reduced assessment of a model’s predictive performance, and hence helps to avoid overfitting.
<!-- Alex suggested to remove this: 
It is important to note that spatial CV reduces the bias introduced by spatial autocorrelation but does not completely remove it. 
This is because there are still a few points in the test and training data which are still neighbors (@brenning_spatial_2012; see second row of \@ref(fig:partitioning)).
--></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:partitioning"></span>
<img src="figures/13_partitioning.png" alt="Spatial visualization of selected test and training observations for cross-validation of one repetition. Random (upper row) and spatial partitioning (lower row)." width="100%"><p class="caption">
FIGURE 12.3: Spatial visualization of selected test and training observations for cross-validation of one repetition. Random (upper row) and spatial partitioning (lower row).
</p>
</div>
</div>
<div id="spatial-cv-with-mlr3" class="section level2" number="12.5">
<h2>
<span class="header-section-number">12.5</span> Spatial CV with <strong>mlr3</strong><a class="anchor" aria-label="anchor" href="#spatial-cv-with-mlr3"><i class="fas fa-link"></i></a>
</h2>
<p>
There are dozens of packages for statistical learning, as described for example in the <a href="https://CRAN.R-project.org/view=MachineLearning">CRAN machine learning task view</a>.
Getting acquainted with each of these packages, including how to undertake cross-validation and hyperparameter tuning, can be a time-consuming process.
Comparing model results from different packages can be even more laborious.
The <strong>mlr3</strong> package and ecosystem was developed to address these issues.
It acts as a ‘meta-package’, providing a unified interface to popular supervised and unsupervised statistical learning techniques including classification, regression, survival analysis and clustering <span class="citation">(<a href="references.html#ref-lang_mlr3_2019" role="doc-biblioref">Lang et al. 2019</a>; <a href="references.html#ref-becker_mlr3_2022" role="doc-biblioref">Becker et al. 2022</a>)</span>.
The standardized <strong>mlr3</strong> interface is based on eight ‘building blocks’.
As illustrated in Figure <a href="spatial-cv.html#fig:building-blocks">12.4</a>, these have a clear order.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:building-blocks"></span>
<img src="figures/13_ml_abstraction_crop.png" alt="Basic building blocks of the mlr3 package. Source: @becker_mlr3_2022. (Permission to reuse this figure was kindly granted.)" width="100%"><p class="caption">
FIGURE 12.4: Basic building blocks of the mlr3 package. Source: <span class="citation">Becker et al. (<a href="references.html#ref-becker_mlr3_2022" role="doc-biblioref">2022</a>)</span>. (Permission to reuse this figure was kindly granted.)
</p>
</div>
<p>The <strong>mlr3</strong> modeling process consists of three main stages.
First, a <strong>task</strong> specifies the data (including response and predictor variables) and the model type (such as regression or classification).
Second, a <strong>learner</strong> defines the specific learning algorithm that is applied to the created task.
Third, the <strong>resampling</strong> approach assesses the predictive performance of the model, i.e., its ability to generalize to new data (see also Section <a href="spatial-cv.html#intro-cv">12.4</a>).</p>
<div id="glm" class="section level3" number="12.5.1">
<h3>
<span class="header-section-number">12.5.1</span> Generalized linear model<a class="anchor" aria-label="anchor" href="#glm"><i class="fas fa-link"></i></a>
</h3>
<p>To implement a GLM in <strong>mlr3</strong>, we must create a <strong>task</strong> containing the landslide data.
Since the response is binary (two-category variable) and has a spatial dimension, we create a classification task with <code>TaskClassifST$new()</code> of the <strong>mlr3spatiotempcv</strong> package <span class="citation">(<a href="references.html#ref-schratz_mlr3spatiotempcv_2021" role="doc-biblioref">Schratz et al. 2021</a>, for non-spatial tasks, use <code>mlr3::TaskClassif$new()</code> or <code>mlr3::TaskRegr$new()</code> for regression tasks, see <code><a href="https://mlr3.mlr-org.com/reference/Task.html">?Task</a></code> for other task types)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The &lt;strong&gt;mlr3&lt;/strong&gt; ecosystem makes heavily use of &lt;strong&gt;data.table&lt;/strong&gt; and &lt;strong&gt;R6&lt;/strong&gt; classes. And though you might use &lt;strong&gt;mlr3&lt;/strong&gt; without knowing the specifics of &lt;strong&gt;data.table&lt;/strong&gt; or &lt;strong&gt;R6&lt;/strong&gt;, it might be rather helpful. To learn more about &lt;strong&gt;data.table&lt;/strong&gt;, please refer to &lt;a href="https://rdatatable.gitlab.io/data.table/index.html" class="uri"&gt;https://rdatatable.gitlab.io/data.table/index.html&lt;/a&gt;. To learn more about &lt;strong&gt;R6&lt;/strong&gt;, we recommend &lt;a href="https://adv-r.hadley.nz/fp.html"&gt;Chapter 14&lt;/a&gt; of the Advanced R book &lt;span class="citation"&gt;(&lt;a href="references.html#ref-wickham_advanced_2019" role="doc-biblioref"&gt;Wickham 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;'><sup>77</sup></a>
The first essential argument of these <code>Task*$new()</code> functions is <code>backend</code>.
<code>backend</code> expects that the input data includes the response and predictor variables.
The <code>target</code> argument indicates the name of a response variable (in our case this is <code>lslpts</code>) and <code>positive</code> determines which of the two factor levels of the response variable indicate the landslide initiation point (in our case this is <code>TRUE</code>).
All other variables of the <code>lsl</code> dataset will serve as predictors.
For spatial CV, we need to provide a few extra arguments (<code>extra_args</code>).
The <code>coordinate_names</code> argument expects the names of the coordinate columns (see Section <a href="spatial-cv.html#intro-cv">12.4</a> and Figure <a href="spatial-cv.html#fig:partitioning">12.3</a>).
Additionally, we should indicate the used CRS (<code>crs</code>) and decide if we want to use the coordinates as predictors in the modeling (<code>coords_as_features</code>).</p>
<div class="sourceCode" id="cb405"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># create task</span></span>
<span><span class="va">task</span> <span class="op">=</span> <span class="fu">mlr3spatiotempcv</span><span class="fu">::</span><span class="va"><a href="https://mlr3spatiotempcv.mlr-org.com/reference/TaskClassifST.html">TaskClassifST</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  id <span class="op">=</span> <span class="st">"ecuador_lsl"</span>,</span>
<span>  backend <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/as_data_backend.html">as_data_backend</a></span><span class="op">(</span><span class="va">lsl</span><span class="op">)</span>, </span>
<span>  target <span class="op">=</span> <span class="st">"lslpts"</span>, </span>
<span>  positive <span class="op">=</span> <span class="st">"TRUE"</span>,</span>
<span>  extra_args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    coordinate_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"x"</span>, <span class="st">"y"</span><span class="op">)</span>,</span>
<span>    coords_as_features <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    crs <span class="op">=</span> <span class="st">"EPSG:32717"</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Note that <code>TaskClassifST$new()</code> also accepts an <code>sf</code>-object as input for the <code>backend</code> parameter.
In this case, you might only want to specify the <code>coords_as_features</code> argument of the <code>extra_args</code> list.
We did not convert <code>lsl</code> into an <code>sf</code>-object because <code>TaskClassifST$new()</code> would just turn it back into a non-spatial <code>data.table</code> object in the background.
For a short data exploration, the <code><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot()</a></code> function of the <strong>mlr3viz</strong> package might come in handy since it plots the response against all predictors and all predictors against all predictors (not shown).</p>
<div class="sourceCode" id="cb406"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># plot response against each predictor</span></span>
<span><span class="fu">mlr3viz</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">task</span>, type <span class="op">=</span> <span class="st">"duo"</span><span class="op">)</span></span>
<span><span class="co"># plot all variables against each other</span></span>
<span><span class="fu">mlr3viz</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/autoplot.html">autoplot</a></span><span class="op">(</span><span class="va">task</span>, type <span class="op">=</span> <span class="st">"pairs"</span><span class="op">)</span></span></code></pre></div>
<p>Having created a task, we need to choose a <strong>learner</strong> that determines the statistical learning method to use.
All classification <strong>learners</strong> start with <code>classif.</code> and all regression learners with <code>regr.</code> (see <code><a href="https://mlr3.mlr-org.com/reference/Learner.html">?Learner</a></code> for details).
<code><a href="https://rdrr.io/pkg/mlr3extralearners/man/list_mlr3learners.html">mlr3extralearners::list_mlr3learners()</a></code> lists all available learners and from which package <strong>mlr3</strong> imports them (Table <a href="spatial-cv.html#tab:lrns">12.2</a>).
To find out about learners that are able to model a binary response variable, we can run:</p>
<div class="sourceCode" id="cb407"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">mlr3extralearners</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/mlr3extralearners/man/list_mlr3learners.html">list_mlr3learners</a></span><span class="op">(</span></span>
<span>  filter <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>class <span class="op">=</span> <span class="st">"classif"</span>, properties <span class="op">=</span> <span class="st">"twoclass"</span><span class="op">)</span>, </span>
<span>  select <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"id"</span>, <span class="st">"mlr3_package"</span>, <span class="st">"required_packages"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/terra/man/headtail.html">head</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:lrns">TABLE 12.2: </span>Sample of available learners for binomial tasks in the mlr3 package.
</caption>
<thead><tr>
<th style="text-align:left;">
id
</th>
<th style="text-align:left;">
mlr3_package
</th>
<th style="text-align:left;">
required_packages
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
classif.AdaBoostM1
</td>
<td style="text-align:left;">
mlr3extralearners
</td>
<td style="text-align:left;">
mlr3 , mlr3extralearners, RWeka
</td>
</tr>
<tr>
<td style="text-align:left;">
classif.bart
</td>
<td style="text-align:left;">
mlr3extralearners
</td>
<td style="text-align:left;">
mlr3 , mlr3extralearners, dbarts
</td>
</tr>
<tr>
<td style="text-align:left;">
classif.C50
</td>
<td style="text-align:left;">
mlr3extralearners
</td>
<td style="text-align:left;">
mlr3 , mlr3extralearners, C50
</td>
</tr>
<tr>
<td style="text-align:left;">
classif.catboost
</td>
<td style="text-align:left;">
mlr3extralearners
</td>
<td style="text-align:left;">
mlr3 , mlr3extralearners, catboost
</td>
</tr>
<tr>
<td style="text-align:left;">
classif.cforest
</td>
<td style="text-align:left;">
mlr3extralearners
</td>
<td style="text-align:left;">
mlr3 , mlr3extralearners, partykit , sandwich , coin
</td>
</tr>
<tr>
<td style="text-align:left;">
classif.ctree
</td>
<td style="text-align:left;">
mlr3extralearners
</td>
<td style="text-align:left;">
mlr3 , mlr3extralearners, partykit , sandwich , coin
</td>
</tr>
</tbody>
</table></div>
<p>This yields all learners able to model two-class problems (landslide yes or no).
We opt for the binomial classification method used in Section <a href="spatial-cv.html#conventional-model">12.3</a> and implemented as <code>classif.log_reg</code> in <strong>mlr3learners</strong>.
Additionally, we need to specify the <code>predict.type</code> which determines the type of the prediction with <code>prob</code> resulting in the predicted probability for landslide occurrence between 0 and 1 (this corresponds to <code>type = response</code> in <code>predict.glm</code>).</p>
<div class="sourceCode" id="cb408"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.log_reg"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span></code></pre></div>
<p>To access the help page of the learner and find out from which package it was taken, we can run:</p>
<div class="sourceCode" id="cb409"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="fu">help</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<!--
Having specified a learner and a task, we can train our model which basically executes the `glm()` command in the background for our task. 


```r
learner$train(task)
learner$model
```




```r
fit = glm(lslpts ~ ., family = binomial(link = "logit"), 
          data = dplyr::select(lsl, -x, -y))
identical(fit$coefficients, learner$model$coefficients)
```
-->
<p>The set-up steps for modeling with <strong>mlr3</strong> may seem tedious.
But remember, this single interface provides access to the 130+ learners shown by <code><a href="https://rdrr.io/pkg/mlr3extralearners/man/list_mlr3learners.html">mlr3extralearners::list_mlr3learners()</a></code>; it would be far more tedious to learn the interface for each learner!
Further advantages are simple parallelization of resampling techniques and the ability to tune machine learning hyperparameters (see Section <a href="spatial-cv.html#svm">12.5.2</a>).
Most importantly, (spatial) resampling in <strong>mlr3spatiotempcv</strong> <span class="citation">(<a href="references.html#ref-schratz_mlr3spatiotempcv_2021" role="doc-biblioref">Schratz et al. 2021</a>)</span> is straightforward, requiring only two more steps: specifying a resampling method and running it.
We will use a 100-repeated 5-fold spatial CV: five partitions will be chosen based on the provided coordinates in our <code>task</code> and the partitioning will be repeated 100 times:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Note that package &lt;strong&gt;sperrorest&lt;/strong&gt; initially implemented spatial cross-validation in R &lt;span class="citation"&gt;(&lt;a href="references.html#ref-brenning_spatial_2012" role="doc-biblioref"&gt;Brenning 2012b&lt;/a&gt;)&lt;/span&gt;.
In the meantime, its functionality was integrated into the &lt;strong&gt;mlr3&lt;/strong&gt; ecosystem which is the reason why we are using &lt;strong&gt;mlr3&lt;/strong&gt; &lt;span class="citation"&gt;(&lt;a href="references.html#ref-schratz_hyperparameter_2019" role="doc-biblioref"&gt;Schratz et al. 2019&lt;/a&gt;)&lt;/span&gt;. The &lt;strong&gt;tidymodels&lt;/strong&gt; framework is another umbrella-package for streamlined modeling in R; however, it only recently integrated support for spatial cross validation via &lt;strong&gt;spatialsample&lt;/strong&gt; which so far only supports one spatial resampling method.&lt;/p&gt;'><sup>78</sup></a></p>
<div class="sourceCode" id="cb410"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">resampling</span> <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"repeated_spcv_coords"</span>, folds <span class="op">=</span> <span class="fl">5</span>, repeats <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<p>To execute the spatial resampling, we run <code><a href="https://mlr3.mlr-org.com/reference/resample.html">resample()</a></code> using the previously specified task, learner, and resampling strategy.
This takes some time (around 15 seconds on a modern laptop) because it computes 500 resampling partitions and 500 models.
As performance measure, we again choose the AUROC.
To retrieve it, we use the <code>score()</code> method of the resampling result output object (<code>score_spcv_glm</code>).
This returns a <code>data.table</code> object with 500 rows – one for each model.</p>
<!--toDo:jn-->
<!--fix pipes-->
<div class="sourceCode" id="cb411"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># reduce verbosity</span></span>
<span><span class="fu">lgr</span><span class="fu">::</span><span class="fu"><a href="https://s-fleck.github.io/lgr/reference/get_logger.html">get_logger</a></span><span class="op">(</span><span class="st">"mlr3"</span><span class="op">)</span><span class="op">$</span><span class="fu">set_threshold</span><span class="op">(</span><span class="st">"warn"</span><span class="op">)</span></span>
<span><span class="co"># run spatial cross-validation and save it to resample result glm (rr_glm)</span></span>
<span><span class="va">rr_spcv_glm</span> <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/resample.html">resample</a></span><span class="op">(</span>task <span class="op">=</span> <span class="va">task</span>,</span>
<span>                             learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>                             resampling <span class="op">=</span> <span class="va">resampling</span><span class="op">)</span></span>
<span><span class="co"># compute the AUROC as a data.table</span></span>
<span><span class="va">score_spcv_glm</span> <span class="op">=</span> <span class="va">rr_spcv_glm</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span>measure <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.auc"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="co"># keep only the columns you need</span></span>
<span>  <span class="va">.</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">task_id</span>, <span class="va">learner_id</span>, <span class="va">resampling_id</span>, <span class="va">classif.auc</span><span class="op">)</span><span class="op">]</span></span></code></pre></div>
<p>The output of the preceding code chunk is a bias-reduced assessment of the model’s predictive performance.
We have saved it as <code>extdata/12-bmr_score.rds</code> in the book’s GitHub repo.
If required, you can read it in as follows:</p>
<div class="sourceCode" id="cb412"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">score</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html">readRDS</a></span><span class="op">(</span><span class="st">"extdata/12-bmr_score.rds"</span><span class="op">)</span></span>
<span><span class="va">score_spcv_glm</span> <span class="op">=</span> <span class="va">score</span><span class="op">[</span><span class="va">learner_id</span> <span class="op">==</span> <span class="st">"classif.log_reg"</span> <span class="op">&amp;</span> </span>
<span>                         <span class="va">resampling_id</span> <span class="op">==</span> <span class="st">"repeated_spcv_coords"</span><span class="op">]</span></span></code></pre></div>
<p>To compute the mean AUROC over all 500 models, we run:</p>
<div class="sourceCode" id="cb413"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/terra/man/summarize-generics.html">mean</a></span><span class="op">(</span><span class="va">score_spcv_glm</span><span class="op">$</span><span class="va">classif.auc</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.77</span></span></code></pre></div>
<p>To put these results in perspective, let us compare them with AUROC values from a 100-repeated 5-fold non-spatial cross-validation (Figure <a href="spatial-cv.html#fig:boxplot-cv">12.5</a>; the code for the non-spatial cross-validation is not shown here but will be explored in the exercise section).
<!--JN: why "as expected"? I think it would be great to explain this expectation in a few sentences here...-->
As expected, the spatially cross-validated result yields lower AUROC values on average than the conventional cross-validation approach, underlining the over-optimistic predictive performance due to spatial autocorrelation of the latter.</p>
<pre><code>#&gt; 
#&gt; Attaching package: 'ggplot2'
#&gt; The following object is masked from 'package:lgr':
#&gt; 
#&gt;     Layout</code></pre>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:boxplot-cv"></span>
<img src="12-spatial-cv_files/figure-html/boxplot-cv-1.png" alt="Boxplot showing the difference in GLM AUROC values on spatial and conventional 100-repeated 5-fold cross-validation." width="75%"><p class="caption">
FIGURE 12.5: Boxplot showing the difference in GLM AUROC values on spatial and conventional 100-repeated 5-fold cross-validation.
</p>
</div>
</div>
<div id="svm" class="section level3" number="12.5.2">
<h3>
<span class="header-section-number">12.5.2</span> Spatial tuning of machine-learning hyperparameters<a class="anchor" aria-label="anchor" href="#svm"><i class="fas fa-link"></i></a>
</h3>
<p>Section <a href="spatial-cv.html#intro-cv">12.4</a> introduced machine learning as part of statistical learning.
To recap, we adhere to the following definition of machine learning by <a href="https://machinelearningmastery.com/linear-regression-for-machine-learning/">Jason Brownlee</a>:</p>
<blockquote>
<p>Machine learning, more specifically the field of predictive modeling, is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability.
In applied machine learning we will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.</p>
</blockquote>
<p>In Section <a href="spatial-cv.html#glm">12.5.1</a> a GLM was used to predict landslide susceptibility.
This section introduces support vector machines (SVM) for the same purpose.
Random forest models might be more popular than SVMs; however, the positive effect of tuning hyperparameters on model performance is much more pronounced in the case of SVMs <span class="citation">(<a href="references.html#ref-probst_hyperparameters_2018" role="doc-biblioref">Probst, Wright, and Boulesteix 2018</a>)</span>.
Since (spatial) hyperparameter tuning is the major aim of this section, we will use an SVM.
For those wishing to apply a random forest model, we recommend to read this chapter, and then proceed to Chapter <a href="eco.html#eco">15</a> in which we will apply the currently covered concepts and techniques to make spatial predictions based on a random forest model.</p>
<p>SVMs search for the best possible ‘hyperplanes’ to separate classes (in a classification case) and estimate ‘kernels’ with specific hyperparameters to create non-linear boundaries between classes <span class="citation">(<a href="references.html#ref-james_introduction_2013" role="doc-biblioref">James et al. 2013</a>)</span>.
Hyperparameters should not be confused with coefficients of parametric models, which are sometimes also referred to as parameters.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;
For a detailed description of the difference between coefficients and hyperparameters, see the ‘machine mastery’ blog post on the subject.
<!-- For a more detailed description of the difference between coefficients and hyperparameters, see the [machine mastery blog](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/). -->&lt;/p&gt;"><sup>79</sup></a>
Coefficients can be estimated from the data, while hyperparameters are set before the learning begins.
Optimal hyperparameters are usually determined within a defined range with the help of cross-validation methods.
This is called hyperparameter tuning.</p>
<p>Some SVM implementations such as that provided by <strong>kernlab</strong> allow hyperparameters to be tuned automatically, usually based on random sampling (see upper row of Figure <a href="spatial-cv.html#fig:partitioning">12.3</a>).
This works for non-spatial data but is of less use for spatial data where ‘spatial tuning’ should be undertaken.</p>
<p>Before defining spatial tuning, we will set up the <strong>mlr3</strong> building blocks, introduced in Section <a href="spatial-cv.html#glm">12.5.1</a>, for the SVM.
The classification task remains the same, hence we can simply reuse the <code>task</code> object created in Section <a href="spatial-cv.html#glm">12.5.1</a>.
Learners implementing SVM can be found using <code>listLearners()</code> as follows:</p>
<div class="sourceCode" id="cb415"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlr3_learners</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/mlr3extralearners/man/list_mlr3learners.html">list_mlr3learners</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">mlr3_learners</span><span class="op">[</span><span class="va">class</span> <span class="op">==</span> <span class="st">"classif"</span> <span class="op">&amp;</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html">grepl</a></span><span class="op">(</span><span class="st">"svm"</span>, <span class="va">id</span><span class="op">)</span>,</span>
<span>              <span class="fu">.</span><span class="op">(</span><span class="va">id</span>, <span class="va">class</span>, <span class="va">mlr3_package</span>, <span class="va">required_packages</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co">#&gt;               id   class      mlr3_package              required_packages</span></span>
<span><span class="co">#&gt; 1:  classif.ksvm classif mlr3extralearners mlr3,mlr3extralearners,kernlab</span></span>
<span><span class="co">#&gt; 2: classif.lssvm classif mlr3extralearners mlr3,mlr3extralearners,kernlab</span></span>
<span><span class="co">#&gt; 3:   classif.svm classif      mlr3learners        mlr3,mlr3learners,e1071</span></span></code></pre></div>
<p>Of the options illustrated above, we will use <code>ksvm()</code> from the <strong>kernlab</strong> package <span class="citation">(<a href="references.html#ref-karatzoglou_kernlab_2004" role="doc-biblioref">Karatzoglou et al. 2004</a>)</span>.
To allow for non-linear relationships, we use the popular radial basis function (or Gaussian) kernel which is also the default of <code>ksvm()</code>.
To make sure that the tuning does not stop because of one failing model, we additionally define a fallback learner (for more information please refer to <a href="https://mlr3book.mlr-org.com/technical.html#fallback-learners" class="uri">https://mlr3book.mlr-org.com/technical.html#fallback-learners</a>).</p>
<div class="sourceCode" id="cb416"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lrn_ksvm</span> <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.ksvm"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span>, kernel <span class="op">=</span> <span class="st">"rbfdot"</span><span class="op">)</span></span>
<span><span class="va">lrn_ksvm</span><span class="op">$</span><span class="va">fallback</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.featureless"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span></code></pre></div>
<p>The next stage is to specify a resampling strategy.
Again we will use a 100-repeated 5-fold spatial CV.</p>
<!-- Instead of saying "outer resampling" we concluded to use "performance estimation level" and "tuning level" (inner) in our paper
# this is also what is shown in the nested CV figure so it would be more consistent -->
<div class="sourceCode" id="cb417"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># performance estimation level</span></span>
<span><span class="va">perf_level</span> <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"repeated_spcv_coords"</span>, folds <span class="op">=</span> <span class="fl">5</span>, repeats <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<p>Note that this is the exact same code as used for the resampling for the GLM in Section <a href="spatial-cv.html#glm">12.5.1</a>; we have simply repeated it here as a reminder.</p>
<p>So far, the process has been identical to that described in Section <a href="spatial-cv.html#glm">12.5.1</a>.
The next step is new, however: to tune the hyperparameters.
Using the same data for the performance assessment and the tuning would potentially lead to overoptimistic results <span class="citation">(<a href="references.html#ref-cawley_overfitting_2010" role="doc-biblioref">Cawley and Talbot 2010</a>)</span>.
This can be avoided using nested spatial CV.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:inner-outer"></span>
<img src="figures/13_cv.png" alt="Schematic of hyperparameter tuning and performance estimation levels in CV. (Figure was taken from Schratz et al. (2019). Permission to reuse it was kindly granted.)" width="100%"><p class="caption">
FIGURE 12.6: Schematic of hyperparameter tuning and performance estimation levels in CV. (Figure was taken from Schratz et al. (2019). Permission to reuse it was kindly granted.)
</p>
</div>
<p>This means that we split each fold again into five spatially disjoint subfolds which are used to determine the optimal hyperparameters (<code>tune_level</code> object in the code chunk below; see Figure <a href="spatial-cv.html#fig:inner-outer">12.6</a> for a visual representation).
To find the optimal hyperparameter combination, we fit 50 models (<code>terminator</code> object in the code chunk below) in each of these subfolds with randomly selected values for the hyperparameters C and Sigma.
The random selection of values C and Sigma is additionally restricted to a predefined tuning space (<code>search_space</code> object).
The range of the tuning space was chosen with values recommended in the literature <span class="citation">(<a href="references.html#ref-schratz_hyperparameter_2019" role="doc-biblioref">Schratz et al. 2019</a>)</span>.</p>
<!--
Questions Pat:
- why not using e1071 svm -> inner hyperparameter tuning also possible I guess...
## Because kernlab has more kernel options. Other than that there is no argument
- explanation correct?
## If you mean the paragraph above, yes
- trafo-function?
## is just a different approach of writing the limits. You could also directly write 2^{-15}. Makes it easier to see the limits at the first glance. Personal preference though
- 125,000 models
-->
<!--
talk in person (see also exercises):
- can I compare the mean AUROC of the GLM and the SVM when using the same seed? Or is seeding not strictly necessary? I mean, ok, the partitions vary a bit but overall...
-->
<div class="sourceCode" id="cb418"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># five spatially disjoint partitions</span></span>
<span><span class="va">tune_level</span> <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"spcv_coords"</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="co"># use 50 randomly selected hyperparameters</span></span>
<span><span class="va">terminator</span> <span class="op">=</span> <span class="fu">mlr3tuning</span><span class="fu">::</span><span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">tuner</span> <span class="op">=</span> <span class="fu">mlr3tuning</span><span class="fu">::</span><span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></span>
<span><span class="co"># define the outer limits of the randomly selected hyperparameters</span></span>
<span><span class="va">search_space</span> <span class="op">=</span> <span class="fu">paradox</span><span class="fu">::</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span></span>
<span>  C <span class="op">=</span> <span class="fu">paradox</span><span class="fu">::</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="op">-</span><span class="fl">12</span>, upper <span class="op">=</span> <span class="fl">15</span>, trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">2</span><span class="op">^</span><span class="va">x</span><span class="op">)</span>,</span>
<span>  sigma <span class="op">=</span> <span class="fu">paradox</span><span class="fu">::</span><span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="op">-</span><span class="fl">15</span>, upper <span class="op">=</span> <span class="fl">6</span>, trafo <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fl">2</span><span class="op">^</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The next stage is to modify the learner <code>lrn_ksvm</code> in accordance with all the characteristics defining the hyperparameter tuning with <code>AutoTuner$new()</code>.</p>
<div class="sourceCode" id="cb419"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">at_ksvm</span> <span class="op">=</span> <span class="fu">mlr3tuning</span><span class="fu">::</span><span class="va"><a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html">AutoTuner</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  learner <span class="op">=</span> <span class="va">lrn_ksvm</span>,</span>
<span>  resampling <span class="op">=</span> <span class="va">tune_level</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.auc"</span><span class="op">)</span>,</span>
<span>  search_space <span class="op">=</span> <span class="va">search_space</span>,</span>
<span>  terminator <span class="op">=</span> <span class="va">terminator</span>,</span>
<span>  tuner <span class="op">=</span> <span class="va">tuner</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The tuning is now set-up to fit 250 models to determine optimal hyperparameters for one fold.
Repeating this for each fold, we end up with 1,250 (250 * 5) models for each repetition.
Repeated 100 times means fitting a total of 125,000 models to identify optimal hyperparameters (Figure <a href="spatial-cv.html#fig:partitioning">12.3</a>).
These are used in the performance estimation, which requires the fitting of another 500 models (5 folds * 100 repetitions; see Figure <a href="spatial-cv.html#fig:partitioning">12.3</a>).
To make the performance estimation processing chain even clearer, let us write down the commands we have given to the computer:</p>
<ol style="list-style-type: decimal">
<li>Performance level (upper left part of Figure <a href="spatial-cv.html#fig:inner-outer">12.6</a>) - split the dataset into five spatially disjoint (outer) subfolds</li>
<li>Tuning level (lower left part of Figure <a href="spatial-cv.html#fig:inner-outer">12.6</a>) - use the first fold of the performance level and split it again spatially into five (inner) subfolds for the hyperparameter tuning.
Use the 50 randomly selected hyperparameters in each of these inner subfolds, i.e., fit 250 models</li>
<li>Performance estimation - Use the best hyperparameter combination from the previous step (tuning level) and apply it to the first outer fold in the performance level to estimate the performance (AUROC)</li>
<li>Repeat steps 2 and 3 for the remaining four outer folds</li>
<li>Repeat steps 2 to 4, 100 times</li>
</ol>
<p>The process of hyperparameter tuning and performance estimation is computationally intensive.
To decrease model runtime, <strong>mlr3</strong> offers the possibility to use parallelization with the help of the <strong>future</strong> package.
Since we are about to run a nested cross-validation, we can decide if we would like to parallelize the inner or the outer loop (see lower left part of Figure <a href="spatial-cv.html#fig:inner-outer">12.6</a>).
Since the former will run 125,000 models, whereas the latter only runs 500, it is quite obvious that we should parallelize the inner loop.
To set up the parallelization of the inner loop, we run:</p>
<div class="sourceCode" id="cb420"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://future.futureverse.org">future</a></span><span class="op">)</span></span>
<span><span class="co"># execute the outer loop sequentially and parallelize the inner loop</span></span>
<span><span class="fu">future</span><span class="fu">::</span><span class="fu"><a href="https://future.futureverse.org/reference/plan.html">plan</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"sequential"</span>, <span class="st">"multisession"</span><span class="op">)</span>, </span>
<span>             workers <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">floor</a></span><span class="op">(</span><span class="fu"><a href="https://parallelly.futureverse.org/reference/availableCores.html">availableCores</a></span><span class="op">(</span><span class="op">)</span> <span class="op">/</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Additionally, we instructed <strong>future</strong> to only use half instead of all available cores (default), a setting that allows possible other users to work on the same high performance computing cluster in case one is used.</p>
<p>Now we are set up for computing the nested spatial CV.
Specifying the <code><a href="https://mlr3.mlr-org.com/reference/resample.html">resample()</a></code> parameters follows the exact same procedure as presented when using a GLM, the only difference being the <code>store_models</code> and <code>encapsulate</code> arguments.
Setting the former to <code>TRUE</code> would allow the extraction of the hyperparameter tuning results which is important if we plan follow-up analyses on the tuning.
The latter ensures that the processing continues even if one of the models throws an error.
This avoids the process stopping just because of one failed model, which is desirable on large model runs.
Once the processing is completed, one can have a look at the failed models.
After the processing, it is good practice to explicitly stop the parallelization with <code>future:::ClusterRegistry("stop")</code>.
Finally, we save the output object (<code>result</code>) to disk in case we would like to use it in another R session.
Before running the subsequent code, be aware that it is time-consuming since it will run the spatial cross-validation with 125,500 models.
Note that runtime depends on many aspects: CPU speed, the selected algorithm, the selected number of cores and the dataset.</p>
<!--toDo:jn-->
<!--fix pipes-->
<div class="sourceCode" id="cb421"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">progressr</span><span class="fu">::</span><span class="fu"><a href="https://progressr.futureverse.org/reference/with_progress.html">with_progress</a></span><span class="op">(</span>expr <span class="op">=</span> <span class="op">{</span></span>
<span>  <span class="va">rr_spcv_svm</span> <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/resample.html">resample</a></span><span class="op">(</span>task <span class="op">=</span> <span class="va">task</span>,</span>
<span>                               learner <span class="op">=</span> <span class="va">at_ksvm</span>, </span>
<span>                               <span class="co"># outer resampling (performance level)</span></span>
<span>                               resampling <span class="op">=</span> <span class="va">perf_level</span>,</span>
<span>                               store_models <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                               encapsulate <span class="op">=</span> <span class="st">"evaluate"</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># stop parallelization</span></span>
<span><span class="fu">future</span><span class="fu">:::</span><span class="fu">ClusterRegistry</span><span class="op">(</span><span class="st">"stop"</span><span class="op">)</span></span>
<span><span class="co"># compute the AUROC values</span></span>
<span><span class="va">score_spcv_svm</span> <span class="op">=</span> <span class="va">rr_spcv_svm</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span>measure <span class="op">=</span> <span class="fu">mlr3</span><span class="fu">::</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.auc"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="co"># keep only the columns you need</span></span>
<span>  <span class="va">.</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">task_id</span>, <span class="va">learner_id</span>, <span class="va">resampling_id</span>, <span class="va">classif.auc</span><span class="op">)</span><span class="op">]</span></span></code></pre></div>
<p>In case you do not want to run the code locally, we have saved <a href="https://github.com/Robinlovelace/geocompr/blob/main/extdata/12-bmr_score.rds">score_svm</a> in the book’s GitHub repo.
They can be loaded as follows:</p>
<div class="sourceCode" id="cb422"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">score</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html">readRDS</a></span><span class="op">(</span><span class="st">"extdata/12-bmr_score.rds"</span><span class="op">)</span></span>
<span><span class="va">score_spcv_svm</span> <span class="op">=</span> <span class="va">score</span><span class="op">[</span><span class="va">learner_id</span> <span class="op">==</span> <span class="st">"classif.ksvm.tuned"</span> <span class="op">&amp;</span> </span>
<span>                         <span class="va">resampling_id</span> <span class="op">==</span> <span class="st">"repeated_spcv_coords"</span><span class="op">]</span></span></code></pre></div>
<p>Let us have a look at the final AUROC: the model’s ability to discriminate the two classes.</p>
<div class="sourceCode" id="cb423"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># final mean AUROC</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/terra/man/summarize-generics.html">mean</a></span><span class="op">(</span><span class="va">score_spcv_svm</span><span class="op">$</span><span class="va">classif.auc</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.74</span></span></code></pre></div>
<p>It appears that the GLM (aggregated AUROC was 0.77) is slightly better than the SVM in this specific case.
To guarantee an absolute fair comparison, one should also make sure that the two models use the exact same partitions – something we have not shown here but have silently used in the background (see <code>code/12_cv.R</code> in the book’s github repo for more information).
To do so, <strong>mlr3</strong> offers the functions <code><a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html">benchmark_grid()</a></code> and <code><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark()</a></code> <span class="citation">(see also <a href="https://mlr3book.mlr-org.com/perf-eval-cmp.html#benchmarking" class="uri" role="doc-biblioref">https://mlr3book.mlr-org.com/perf-eval-cmp.html#benchmarking</a>, <a href="references.html#ref-becker_mlr3_2022" role="doc-biblioref">Becker et al. 2022</a>)</span>.
We will explore these functions in more detail in the Exercises.
Please note also that using more than 50 iterations in the random search of the SVM would probably yield hyperparameters that result in models with a better AUROC <span class="citation">(<a href="references.html#ref-schratz_hyperparameter_2019" role="doc-biblioref">Schratz et al. 2019</a>)</span>.
On the other hand, increasing the number of random search iterations would also increase the total number of models and thus runtime.</p>
<p>So far spatial CV has been used to assess the ability of learning algorithms to generalize to unseen data.
For spatial predictions, one would tune the hyperparameters on the complete dataset.
This will be covered in Chapter <a href="eco.html#eco">15</a>.</p>
</div>
</div>
<div id="conclusions" class="section level2" number="12.6">
<h2>
<span class="header-section-number">12.6</span> Conclusions<a class="anchor" aria-label="anchor" href="#conclusions"><i class="fas fa-link"></i></a>
</h2>
<p>Resampling methods are an important part of a data scientist’s toolbox <span class="citation">(<a href="references.html#ref-james_introduction_2013" role="doc-biblioref">James et al. 2013</a>)</span>.
This chapter used cross-validation to assess predictive performance of various models.
As described in Section <a href="spatial-cv.html#intro-cv">12.4</a>, observations with spatial coordinates may not be statistically independent due to spatial autocorrelation, violating a fundamental assumption of cross-validation.
Spatial CV addresses this issue by reducing bias introduced by spatial autocorrelation.</p>
<p>The <strong>mlr3</strong> package facilitates (spatial) resampling techniques in combination with the most popular statistical learning techniques including linear regression, semi-parametric models such as generalized additive models and machine learning techniques such as random forests, SVMs, and boosted regression trees <span class="citation">(<a href="references.html#ref-bischl_mlr:_2016" role="doc-biblioref">Bischl et al. 2016</a>; <a href="references.html#ref-schratz_hyperparameter_2019" role="doc-biblioref">Schratz et al. 2019</a>)</span>.
Machine learning algorithms often require hyperparameter inputs, the optimal ‘tuning’ of which can require thousands of model runs which require large computational resources, consuming much time, RAM and/or cores.
<strong>mlr3</strong> tackles this issue by enabling parallelization.</p>
<p>Machine learning overall, and its use to understand spatial data, is a large field and this chapter has provided the basics, but there is more to learn.
We recommend the following resources in this direction:</p>
<ul>
<li>The <strong>mlr3 book</strong> [<span class="citation">Becker et al. (<a href="references.html#ref-becker_mlr3_2022" role="doc-biblioref">2022</a>)</span>; <a href="https://mlr-org.github.io/mlr-tutorial/release/html/" class="uri">https://mlr-org.github.io/mlr-tutorial/release/html/</a>] and especially the <a href="https://mlr3book.mlr-org.com/spatiotemporal.html">chapter on the handling of spatio-temporal data</a>
</li>
<li>An academic paper on hyperparameter tuning <span class="citation">(<a href="references.html#ref-schratz_hyperparameter_2019" role="doc-biblioref">Schratz et al. 2019</a>)</span>
</li>
<li>An academic paper on how to use <strong>mlr3spatiotempcv</strong> <span class="citation">(<a href="references.html#ref-schratz_mlr3spatiotempcv_2021" role="doc-biblioref">Schratz et al. 2021</a>)</span>
</li>
<li>In case of spatio-temporal data, one should account for spatial and temporal autocorrelation when doing CV <span class="citation">(<a href="references.html#ref-meyer_improving_2018" role="doc-biblioref">Meyer et al. 2018</a>)</span>
</li>
</ul>
</div>
<div id="exercises-9" class="section level2" number="12.7">
<h2>
<span class="header-section-number">12.7</span> Exercises<a class="anchor" aria-label="anchor" href="#exercises-9"><i class="fas fa-link"></i></a>
</h2>
<p>E1. Compute the following terrain attributes from the <code>elev</code> dataset loaded with <code>terra::rast(system.file("raster/ta.tif", package = "spDataLarge"))$elev</code> with the help of R-GIS bridges (see this <a href="https://geocompr.robinlovelace.net/gis.html#gis">Chapter</a>):</p>
<pre><code>- Slope
- Plan curvature
- Profile curvature
- Catchment area</code></pre>
<p>E2. Extract the values from the corresponding output rasters to the <code>lsl</code> data frame (<code>data("lsl", package = "spDataLarge"</code>) by adding new variables called <code>slope</code>, <code>cplan</code>, <code>cprof</code>, <code>elev</code> and <code>log_carea</code> (see this <a href="https://geocompr.robinlovelace.net/spatial-cv.html#case-landslide">section</a> for details).</p>
<p>E3. Use the derived terrain attribute rasters in combination with a GLM to make a spatial prediction map similar to that shown in this <a href="https://geocompr.robinlovelace.net/spatial-cv.html#fig:lsl-susc">Figure</a>.
Running <code>data("study_mask", package = "spDataLarge")</code> attaches a mask of the study area.</p>
<p>E4. Compute a 100-repeated 5-fold non-spatial cross-validation and spatial CV based on the GLM learner and compare the AUROC values from both resampling strategies with the help of boxplots (see this <a href="https://geocompr.robinlovelace.net/spatial-cv.html#fig:boxplot-cv">Figure</a>).</p>
<p>Hint: You need to specify a non-spatial resampling strategy.</p>
<p>Another hint: You might want to solve Excercises 4 to 6 in one go with the help of <code><a href="https://mlr3.mlr-org.com/reference/benchmark.html">mlr3::benchmark()</a></code> and <code><a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html">mlr3::benchmark_grid()</a></code> (for more information, please refer to <a href="https://mlr3book.mlr-org.com/perf-eval-cmp.html#benchmarking" class="uri">https://mlr3book.mlr-org.com/perf-eval-cmp.html#benchmarking</a>).
When doing so, keep in mind that the computation can take very long, probably several days.
This, of course, depends on your system.
Computation time will be shorter the more RAM and cores you have at your disposal.</p>
<p>E5. Model landslide susceptibility using a quadratic discriminant analysis (QDA).
Assess the predictive performance of the QDA.
What is the a difference between the spatially cross-validated mean AUROC value of the QDA and the GLM?</p>
<p>E6. Run the SVM without tuning the hyperparameters.
Use the <code>rbfdot</code> kernel with <span class="math inline">\(\sigma\)</span> = 1 and <em>C</em> = 1.
Leaving the hyperparameters unspecified in <strong>kernlab</strong>’s <code>ksvm()</code> would otherwise initialize an automatic non-spatial hyperparameter tuning.</p>

</div>
</div>




  <div class="chapter-nav">
<div class="prev"><a href="algorithms.html"><span class="header-section-number">11</span> Scripts, algorithms and functions</a></div>
<div class="next"><a href="transport.html"><span class="header-section-number">13</span> Transportation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <h2>Note: Second Edition is under construction 🏗</h2>
    <!--<p>Now is a great time to provide feedback</p>-->
        <ul class="list-unstyled">
<!--<li><a href="https://forms.gle/nq9RmbxJyZXQgc948">Provide feedback (5 min)</a></li>--><li><a href="https://geocompr.robinlovelace.net/#reproducibility">Install updated packages</a></li>
          <li><a href="https://github.com/Robinlovelace/geocompr/issues">Open an issue <i class="fas fa-question"></i></a></li>
          <li><a href="https://discord.gg/PMztXYgNxp">Chat on Discord <i class="fab fa-discord"></i></a></li>
          <li><a href="https://supportukrainenow.org/">Support Ukraine 🇺🇦
</a></li>
        </ul>
<hr>
<nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#spatial-cv"><span class="header-section-number">12</span> Statistical learning</a></li>
<li><a class="nav-link" href="#prerequisites-9">Prerequisites</a></li>
<li><a class="nav-link" href="#intro-cv1"><span class="header-section-number">12.1</span> Introduction</a></li>
<li><a class="nav-link" href="#case-landslide"><span class="header-section-number">12.2</span> Case study: Landslide susceptibility</a></li>
<li><a class="nav-link" href="#conventional-model"><span class="header-section-number">12.3</span> Conventional modeling approach in R</a></li>
<li><a class="nav-link" href="#intro-cv"><span class="header-section-number">12.4</span> Introduction to (spatial) cross-validation</a></li>
<li>
<a class="nav-link" href="#spatial-cv-with-mlr3"><span class="header-section-number">12.5</span> Spatial CV with mlr3</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#glm"><span class="header-section-number">12.5.1</span> Generalized linear model</a></li>
<li><a class="nav-link" href="#svm"><span class="header-section-number">12.5.2</span> Spatial tuning of machine-learning hyperparameters</a></li>
</ul>
</li>
<li><a class="nav-link" href="#conclusions"><span class="header-section-number">12.6</span> Conclusions</a></li>
<li><a class="nav-link" href="#exercises-9"><span class="header-section-number">12.7</span> Exercises</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/Robinlovelace/geocompr/blob/main/12-spatial-cv.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/Robinlovelace/geocompr/edit/main/12-spatial-cv.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Geocomputation with R</strong>" was written by Robin Lovelace, Jakub Nowosad, Jannes Muenchow. It was last built on 2022-08-08.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
